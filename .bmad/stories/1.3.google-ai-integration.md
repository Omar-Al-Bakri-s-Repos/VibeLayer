# Story 1.3: Google AI Integration & Speech Processing

## Status: Draft
**Priority:** 2 (Core AI Feature)
**Depends On:** Stories 1.1, 1.2 (needs infrastructure and WebSocket)

## Story Statement
**As a** creator,
**I want** the system to listen to my conversation and understand what's happening,
**So that** relevant effects can be suggested automatically during my stream.

## Acceptance Criteria
1. ✅ Google Cloud STT v2 integration with streaming capabilities and voice activity detection
2. ✅ Speaker diarization distinguishes between host and guests with unique identifiers
3. ✅ Music/speech separation prevents background music from triggering false suggestions
4. ✅ Audio stream encryption implemented for privacy protection
5. ✅ Basic audio processing achieves <300ms STT latency on stable network connections
6. ✅ Transcript confidence scoring provides reliability metrics for downstream processing

## Dev Notes

### Epic Context
This story implements the audio input pipeline that feeds the AI system. It's the entry point for all conversation data that drives effect suggestions. Critical for the AI intelligence that differentiates VibeLayerAI.

### Story Dependencies
- **Depends On:** Stories 1.1, 1.2 (infrastructure and WebSocket)
- **Blocks:** Story 1.4 (intent classification needs transcripts)
- **Parallel Potential:** Can work alongside Stories 1.5, 1.6

### Technical Architecture

#### Google Cloud Integration
- **Service:** Google Cloud Speech-to-Text v2
- **Mode:** Streaming recognition for real-time processing
- **Features:** Speaker diarization, punctuation, profanity filter
- **Audio:** 16kHz, 16-bit PCM, mono channel

#### Audio Pipeline
```typescript
// From control panel microphone
AudioStream → WebSocket → Server → Google STT → Transcript → Intent Engine

interface AudioMessage {
  type: 'audio_chunk';
  sessionId: string;
  chunk: ArrayBuffer;  // 16-bit PCM audio
  sampleRate: 16000;
}

interface TranscriptResult {
  text: string;
  confidence: number;
  speaker: string;
  isFinal: boolean;
  timestamp: Date;
}
```

### Implementation Tasks

#### Task 1: Set Up Google Cloud STT
- [ ] Configure Google Cloud project and enable STT API
- [ ] Set up service account with proper permissions
- [ ] Store credentials securely in Doppler
- [ ] Install @google-cloud/speech SDK
- [ ] Create STT service wrapper

#### Task 2: Implement Audio Streaming
- [ ] Create audio processing service in apps/api-services/src/services/ai/
- [ ] Set up streaming recognition client
- [ ] Configure recognition settings (diarization, language)
- [ ] Handle streaming lifecycle (start/stop/restart)
- [ ] Implement audio buffering for reliability

#### Task 3: Add Speaker Diarization
- [ ] Enable speaker diarization in STT config
- [ ] Create speaker identification system
- [ ] Map speaker IDs to user-friendly labels
- [ ] Track speaker changes in transcripts
- [ ] Store speaker profiles per session

#### Task 4: Implement Music/Speech Separation
- [ ] Add voice activity detection (VAD)
- [ ] Implement audio level monitoring
- [ ] Create background noise filter
- [ ] Detect and filter music segments
- [ ] Add confidence thresholds for filtering

#### Task 5: Add Audio Encryption
- [ ] Implement E2E encryption for audio stream
- [ ] Use AES-256-GCM for audio chunks
- [ ] Create key exchange mechanism
- [ ] Add encryption to WebSocket messages
- [ ] Ensure HIPAA compliance considerations

#### Task 6: Optimize for Low Latency
- [ ] Configure optimal chunk size (100ms)
- [ ] Implement parallel processing pipeline
- [ ] Add result caching for repeated phrases
- [ ] Optimize network packet size
- [ ] Monitor and log latency metrics

#### Task 7: Create Transcript Management
- [ ] Store transcripts in Convex with session ID
- [ ] Implement rolling transcript window (last 60s)
- [ ] Add confidence scoring system
- [ ] Create transcript correction mechanism
- [ ] Build transcript export functionality

#### Task 8: Add Client-Side Audio Capture
- [ ] Create audio capture hook in control panel
- [ ] Implement Web Audio API integration
- [ ] Add microphone permission handling
- [ ] Create audio level visualization
- [ ] Handle browser compatibility

### File Locations
- STT service: `/apps/api-services/src/services/ai/speech/`
- Audio processing: `/apps/api-services/src/services/ai/audio/`
- Client audio: `/apps/control-panel/src/hooks/useAudioCapture.ts`
- Transcript storage: `/apps/api-services/src/models/transcript.ts`
- WebSocket handlers: `/apps/api-services/src/websocket/handlers/audio.ts`

### Testing Requirements
- Unit tests for audio processing pipeline
- Integration tests with Google STT API
- Latency benchmarks (<300ms p95)
- Speaker diarization accuracy tests
- Music filtering effectiveness tests
- Load tests with concurrent audio streams
- Privacy/encryption validation

### Technical Constraints
- STT latency must be <300ms (p95)
- Support 100+ concurrent audio streams
- Audio chunk size: 100ms (1600 samples at 16kHz)
- Maximum session length: 8 hours
- Transcript retention: 24 hours

## Definition of Done
- [ ] Google STT integrated and configured
- [ ] Audio streaming from browser to server working
- [ ] Speaker diarization identifying multiple speakers
- [ ] Music/speech separation preventing false triggers
- [ ] Audio encryption implemented end-to-end
- [ ] Latency consistently <300ms
- [ ] Transcripts stored and accessible
- [ ] Confidence scoring implemented
- [ ] Integration tests passing
- [ ] Documentation updated